{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting tissue specific eQTL summary statistics to HDF5\n",
    "Here I convert GTEx summary statistics to HDF5 format, making it easier to query and share the results. \n",
    "\n",
    "This procedure was initially written and used for GTEx V6 data in 2015. It is now updated to deal with V8 data, for input to `mashr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "Summar statistics data are in BED format, one row per gene-snp pair. Each tissue has a separate BED file. Additionally there are support files of gene transcription start site coordinates, and SNP coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to HDF5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark from previous runs:\n",
    "\n",
    "| time | original size | HDF5 size | method | data type |\n",
    "|:------:|:-------:|:-------:|:-------:|:-----:|\n",
    "| 118 min| 5G (179,083,485 lines) | 3.8G | TBData, bzip2 | float32|  \n",
    "| 118 min| 5G (179,083,485 lines) | 5.1G | TBData, bzip2 | float64|\n",
    "| 138 min| 5G (179,083,485 lines) | 4.9G | TBData, zlib | float32|\n",
    "| 39 min| 5G (179,083,485 lines) | 6.9G | HPData, zlib | float64|\n",
    "\n",
    "I will use `bzip2` compression and `float32` to make initial compression, then extract for `mashr` input and save to RDS format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "cwd = '~/Documents/GTEx/V8'\n",
    "sumstats_files = glob.glob(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data table classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[sumstats_to_hdf5]\n",
    "depends: executable(\"h5copy\")\n",
    "parameter: msg = \"GTEX V8 fastqtl analysis\"\n",
    "parameter: maxsize = 1000 # maximum number of groups per HDF5 file\n",
    "input: sumstats_files, group_by = 1, pattern = \"{name}.gz\"\n",
    "output: expand_pattern('{_name}.h5')\n",
    "task:\n",
    "python:\n",
    "import sys, os\n",
    "import numpy as np, pandas as pd, tables as tb\n",
    "tb.parameters.MAX_GROUP_WIDTH = 51200\n",
    "# tb.parameters.NODE_CACHE_SLOTS = -51200\n",
    "# tb.parameters.METADATA_CACHE_SIZE = 1048576 * 100000\n",
    "# tb.parameters.CHUNK_CACHE_SIZE = 2097152 * 100000\n",
    "# tb.parameters.CHUNK_CACHE_NELMTS = 521\n",
    "\n",
    "class TBData(dict):\n",
    "    def __init__(self, data, name, msg = None, root = '/', complib = 'bzip2'):\n",
    "        '''bzip2 may not be compatible with other hdf5 applications; but zlib is fine'''\n",
    "        self.__root = root.strip('/')\n",
    "        self.__group = name\n",
    "        self.__msg = msg\n",
    "        try:\n",
    "            if type(data) is dict:\n",
    "                self.update(data)\n",
    "            elif type(data) is str:\n",
    "                # is file name\n",
    "                self.__load(tb.open_file(data))\n",
    "            else:\n",
    "                # is file stream\n",
    "                self.__load(data)\n",
    "        except tb.exceptions.NoSuchNodeError:\n",
    "            raise ValueError('Cannot find dataset {}!'.format(name))\n",
    "        self.tb_filters = tb.Filters(complevel=9, complib=complib)\n",
    "\n",
    "    def sink(self, filename):\n",
    "        with tb.open_file(filename, 'a') as f:\n",
    "            if self.__root:\n",
    "                try:\n",
    "                    f.create_group(\"/\", self.__root)\n",
    "                except:\n",
    "                    pass\n",
    "            try:\n",
    "                # there is existing data -- have to merge with current data\n",
    "                # have to do this because the input file lines are not grouped by gene names!!\n",
    "                # use try ... except to hopefully faster than if ... else\n",
    "                # e.g., if not f.__contains__('/{}'.format(self.__group)) ... else ...\n",
    "                for element in f.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                    if element.name != 'colnames':\n",
    "                        self[element.name] = np.concatenate((element[:], self[element.name]))\n",
    "            except tb.exceptions.NoSuchNodeError:\n",
    "                f.create_group(\"/\" + self.__root, self.__group,\n",
    "                               self.__msg if self.__msg else self.__group)\n",
    "            for key in self:\n",
    "                self.__store_array(key, f)\n",
    "            f.flush()\n",
    "\n",
    "    def dump(self, table, output = False):\n",
    "        if output:\n",
    "            pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames']).to_csv(sys.stdout, na_rep = 'NA')\n",
    "            return None\n",
    "        else:\n",
    "            return pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames'])\n",
    "\n",
    "    def __load(self, fstream):\n",
    "        try:\n",
    "            for element in fstream.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                self[element.name] = element[:]\n",
    "            fstream.close()\n",
    "        except:\n",
    "            fstream.close()\n",
    "            raise\n",
    "\n",
    "    def __roll_back(self, group, name):\n",
    "        try:\n",
    "            n = getattr(group, name)\n",
    "            n._f_remove()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def __store_array(self, name, fstream):\n",
    "        if self.__root:\n",
    "            element = getattr(getattr(fstream.root, self.__root), self.__group)\n",
    "        else:\n",
    "            element = getattr(fstream.root, self.__group)\n",
    "        arr = self[name]\n",
    "        if type(arr) is list:\n",
    "            arr = np.array(arr)\n",
    "        self.__roll_back(element, name)\n",
    "        #\n",
    "        if arr.shape != (0,):\n",
    "            ds = fstream.create_carray(element, name, tb.Atom.from_dtype(arr.dtype), arr.shape,\n",
    "                                       filters = self.tb_filters)\n",
    "            ds[:] = arr\n",
    "\n",
    "def get_tb_grps(filenames, group_name = None):\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "    names = set()\n",
    "    for filename in filenames:\n",
    "        with tb.open_file(filename) as f:\n",
    "            names.update([node._v_name for node in (f.root if group_name is None else getattr(f.root, '{}'.format(group_name)))])\n",
    "    return sorted(names)\n",
    "\n",
    "class SSData:\n",
    "    def __init__(self, header = False):\n",
    "        self.data = {'buffer':{'data':[], 'rownames':[]}, 'output':{}}\n",
    "        self.header = header\n",
    "        self.previous_name = self.current_name = None\n",
    "        self.count = -1\n",
    "\n",
    "    def parse(self, line):\n",
    "        # input line is snp, gene, beta, t, pval\n",
    "        if not line:\n",
    "            self.__reset()\n",
    "            self.current_name = None\n",
    "            return 1\n",
    "        line = line.strip().split()\n",
    "        self.count += 1\n",
    "        if self.header and self.count == 0:\n",
    "            return 0\n",
    "        #\n",
    "        if self.previous_name is None:\n",
    "            self.previous_name = line[1]\n",
    "        self.current_name = line[1]\n",
    "        if self.current_name != self.previous_name:\n",
    "            self.__reset()\n",
    "        self.data['buffer']['data'].append([line[2], line[3], line[4]])\n",
    "        self.data['buffer']['rownames'].append(line[0])\n",
    "        return 0\n",
    "\n",
    "    def __reset(self):\n",
    "        self.data['buffer']['data'] = np.array(self.data['buffer']['data'], dtype = np.float32)\n",
    "        self.data['buffer']['rownames'] = np.array(self.data['buffer']['rownames'])\n",
    "        self.data['buffer']['colnames'] = np.array(['beta','t-stat','p-value'])\n",
    "        self.data['output'] = copy.deepcopy(self.data['buffer'])\n",
    "        self.data['buffer'] = {'data':[], 'rownames':[]}\n",
    "\n",
    "    def dump(self):\n",
    "        return self.data['output']\n",
    "\n",
    "ssp = SSData(header = True)\n",
    "group_counts = 0\n",
    "bname = os.path.basename(item)\n",
    "with gzip.open(${_input!r}) as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        quit = ssp.parse(line)\n",
    "        if ssp.current_name != ssp.previous_name:\n",
    "            group_counts += 1\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", category = tb.NaturalNameWarning)\n",
    "                # warnings.filterwarnings(\"ignore\", category = tb.PerformanceWarning)\n",
    "                data = TBData(ssp.dump(), ssp.previous_name, ${msg!r})\n",
    "                data.sink(\"${_output!n}_%i.tmp\" % (np.ceil(group_counts / ${maxsize})) if ${maxsize} > 0 else ${_output!r})\n",
    "            ssp.previous_name = ssp.current_name\n",
    "\n",
    "if ${maxsize} > 0:\n",
    "    from glob import glob\n",
    "    tmpfiles = list(glob(\"${_output!n}_*.tmp\"))\n",
    "    for item in sorted(tmpfiles):\n",
    "        for name in get_tb_grps(item):\n",
    "            os.system('h5copy -i {0} -o ${_output} -s \"/{1}\" -d \"/{1}\"'.format(item, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos.jupyter.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "Python3",
     "python3",
     "Python3",
     "#FFE771"
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": false,
    "height": 0,
    "style": "side"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
